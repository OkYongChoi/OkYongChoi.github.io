---
# permalink: /posts/
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
toc: true
categories:
  - natural language processing
tags:
  - attention mechanism
  - transformer
  - masked language model
  - pre-trained model
  - bert


---
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova
