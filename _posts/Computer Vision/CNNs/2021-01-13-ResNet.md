# Going Deeper with Convolutions (ResNet)
https://arxiv.org/abs/1409.4842

In the ResNet, an incredibly deep structure of 152 layers was used, and new concepts such as residual learning, shortcut connection, and identity mapping were introduced. It is confirmed that the **shortcut** structure rather than the simple feed forward structure is the correct answer.

The residual connection (skip connection) is also used in the transformer